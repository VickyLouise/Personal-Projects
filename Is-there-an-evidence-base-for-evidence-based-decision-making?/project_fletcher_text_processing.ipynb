{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This file transforms each speech into a TF-IDF matrix, reduces the dimensionality and calculates the similarity of each speech to the Royal Institution Christmas Lecture speeches I identified as being good examples of evidence-based speeches.\n",
    "\n",
    "Please see more information about the RI Christmas Lectures here: http://www.bbc.co.uk/programmes/b00pmbqq\n",
    "\n",
    "They are described as a \"Series of lectures on a single topic, presenting scientific subjects to a general audience in an informative and entertaining manner\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I load the data I pickled from 'project_fletcher_cleaning'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "pkl_file = open('df_all_science_docs.pkl', 'rb')\n",
    "df_all_science_docs = pickle.load(pkl_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemming\n",
    "\n",
    "I stem all of the words in the each of the speeches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.snowball import SnowballStemmer\n",
    "stemmer = SnowballStemmer('english')\n",
    "stemmed =[' '.join([stemmer.stem(word) for word in str(text).split(' ')])\n",
    "          for text in df_all_science_docs['speech']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = open('stemmed.pkl', 'wb')\n",
    "pickle.dump(stemmed, output)\n",
    "\n",
    "output.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmetising\n",
    "\n",
    "I then lemmetise the stemmed words (unnecessary to do both - since I do not need to generate text, I can probably stick with just the stemmed words => I will try this)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "pkl_file = open('stemmed.pkl', 'rb')\n",
    "stemmed = pickle.load(pkl_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/ubuntu/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmer=WordNetLemmatizer()\n",
    "lemmatised=[' '.join([lemmer.lemmatize(word) for word in str(text).split(' ')])\n",
    "          for text in stemmed]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = open('lemmatised.pkl', 'wb')\n",
    "pickle.dump(lemmatised, output)\n",
    "\n",
    "output.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF\n",
    "\n",
    "I convert the speeches of lemmatised words into a TF-IDF vector. I did some tuning of ngrams, maximum and minimum document frequency, and the maximum number of features.  I would like to do some further tuning of this. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "pkl_file = open('lemmatised.pkl', 'rb')\n",
    "lemmatised = pickle.load(pkl_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(823989, 1569)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>abil</th>\n",
       "      <th>abl</th>\n",
       "      <th>abov</th>\n",
       "      <th>absolut</th>\n",
       "      <th>absolut right</th>\n",
       "      <th>abus</th>\n",
       "      <th>accept</th>\n",
       "      <th>access</th>\n",
       "      <th>accommod</th>\n",
       "      <th>accord</th>\n",
       "      <th>...</th>\n",
       "      <th>year</th>\n",
       "      <th>year ago</th>\n",
       "      <th>year old</th>\n",
       "      <th>years</th>\n",
       "      <th>yes</th>\n",
       "      <th>yesterday</th>\n",
       "      <th>young</th>\n",
       "      <th>young peopl</th>\n",
       "      <th>young people</th>\n",
       "      <th>youth</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.125927</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 1569 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   abil  abl  abov  absolut  absolut right  abus  accept  access  accommod  \\\n",
       "0   0.0  0.0   0.0      0.0            0.0   0.0     0.0     0.0       0.0   \n",
       "1   0.0  0.0   0.0      0.0            0.0   0.0     0.0     0.0       0.0   \n",
       "2   0.0  0.0   0.0      0.0            0.0   0.0     0.0     0.0       0.0   \n",
       "3   0.0  0.0   0.0      0.0            0.0   0.0     0.0     0.0       0.0   \n",
       "4   0.0  0.0   0.0      0.0            0.0   0.0     0.0     0.0       0.0   \n",
       "\n",
       "   accord  ...        year  year ago  year old  years  yes  yesterday  young  \\\n",
       "0     0.0  ...    0.000000       0.0       0.0    0.0  0.0        0.0    0.0   \n",
       "1     0.0  ...    0.000000       0.0       0.0    0.0  0.0        0.0    0.0   \n",
       "2     0.0  ...    0.125927       0.0       0.0    0.0  0.0        0.0    0.0   \n",
       "3     0.0  ...    0.000000       0.0       0.0    0.0  0.0        0.0    0.0   \n",
       "4     0.0  ...    0.000000       0.0       0.0    0.0  0.0        0.0    0.0   \n",
       "\n",
       "   young peopl  young people  youth  \n",
       "0          0.0           0.0    0.0  \n",
       "1          0.0           0.0    0.0  \n",
       "2          0.0           0.0    0.0  \n",
       "3          0.0           0.0    0.0  \n",
       "4          0.0           0.0    0.0  \n",
       "\n",
       "[5 rows x 1569 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "tfidf = TfidfVectorizer(lowercase = True,\n",
    "                        stop_words = \"english\",\n",
    "                        ngram_range=(1,2),\n",
    "                        token_pattern=\"\\\\b[a-zA-Z][a-zA-Z]+\\\\b\", #words with >= 2 alpha chars \n",
    "                        min_df=0.0075,\n",
    "                       max_df=0.8,\n",
    "                       max_features=5000)\n",
    "tfidf_vecs = tfidf.fit_transform(lemmatised)\n",
    "df_tfidf = pd.DataFrame(tfidf_vecs.todense(), \n",
    "             columns=tfidf.get_feature_names())\n",
    "print(df_tfidf.shape)\n",
    "df_tfidf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pickle model\n",
    "from sklearn.externals import joblib\n",
    "joblib.dump(tfidf,'tfidf.pkl')\n",
    "\n",
    "# Pickle the vectors\n",
    "output = open('tfidf_vecs.pkl', 'wb')\n",
    "pickle.dump(tfidf_vecs, output)\n",
    "\n",
    "output.close()\n",
    "\n",
    "# Pickle dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSI with gensim\n",
    "\n",
    "The data is high-dimensional and comparisons of similarity are likely to be more fruitful with dimensionality reduction. I reduce the number of dimensions using LSI. Since I'm not interested in having interpretable topics per se, I chose LSI instead of NMF as it's faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.externals import joblib\n",
    "import pickle\n",
    "# Load the model\n",
    "tfidf = joblib.load('tfidf.pkl')\n",
    "\n",
    "# Load the vectors\n",
    "pkl_file = open('tfidf_vecs.pkl', 'rb')\n",
    "tfidf_vecs= pickle.load(pkl_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting gensim\n",
      "  Downloading gensim-3.4.0-cp36-cp36m-manylinux1_x86_64.whl (22.6MB)\n",
      "\u001b[K    100% |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 22.6MB 60kB/s  eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: six>=1.5.0 in /home/ubuntu/anaconda3/lib/python3.6/site-packages (from gensim)\n",
      "Collecting smart-open>=1.2.1 (from gensim)\n",
      "  Downloading smart_open-1.5.6.tar.gz\n",
      "Requirement already satisfied: scipy>=0.18.1 in /home/ubuntu/anaconda3/lib/python3.6/site-packages (from gensim)\n",
      "Requirement already satisfied: numpy>=1.11.3 in /home/ubuntu/anaconda3/lib/python3.6/site-packages (from gensim)\n",
      "Requirement already satisfied: boto>=2.32 in /home/ubuntu/anaconda3/lib/python3.6/site-packages (from smart-open>=1.2.1->gensim)\n",
      "Collecting bz2file (from smart-open>=1.2.1->gensim)\n",
      "  Downloading bz2file-0.98.tar.gz\n",
      "Requirement already satisfied: requests in /home/ubuntu/anaconda3/lib/python3.6/site-packages (from smart-open>=1.2.1->gensim)\n",
      "Collecting boto3 (from smart-open>=1.2.1->gensim)\n",
      "  Downloading boto3-1.6.6-py2.py3-none-any.whl (128kB)\n",
      "\u001b[K    100% |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 133kB 8.9MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: chardet<3.1.0,>=3.0.2 in /home/ubuntu/anaconda3/lib/python3.6/site-packages (from requests->smart-open>=1.2.1->gensim)\n",
      "Requirement already satisfied: idna<2.7,>=2.5 in /home/ubuntu/anaconda3/lib/python3.6/site-packages (from requests->smart-open>=1.2.1->gensim)\n",
      "Requirement already satisfied: urllib3<1.23,>=1.21.1 in /home/ubuntu/anaconda3/lib/python3.6/site-packages (from requests->smart-open>=1.2.1->gensim)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ubuntu/anaconda3/lib/python3.6/site-packages (from requests->smart-open>=1.2.1->gensim)\n",
      "Collecting botocore<1.10.0,>=1.9.6 (from boto3->smart-open>=1.2.1->gensim)\n",
      "  Downloading botocore-1.9.6-py2.py3-none-any.whl (4.1MB)\n",
      "\u001b[K    100% |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4.1MB 347kB/s eta 0:00:01\n",
      "\u001b[?25hCollecting jmespath<1.0.0,>=0.7.1 (from boto3->smart-open>=1.2.1->gensim)\n",
      "  Downloading jmespath-0.9.3-py2.py3-none-any.whl\n",
      "Collecting s3transfer<0.2.0,>=0.1.10 (from boto3->smart-open>=1.2.1->gensim)\n",
      "  Downloading s3transfer-0.1.13-py2.py3-none-any.whl (59kB)\n",
      "\u001b[K    100% |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 61kB 11.1MB/s ta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: python-dateutil<3.0.0,>=2.1 in /home/ubuntu/anaconda3/lib/python3.6/site-packages (from botocore<1.10.0,>=1.9.6->boto3->smart-open>=1.2.1->gensim)\n",
      "Requirement already satisfied: docutils>=0.10 in /home/ubuntu/anaconda3/lib/python3.6/site-packages (from botocore<1.10.0,>=1.9.6->boto3->smart-open>=1.2.1->gensim)\n",
      "Building wheels for collected packages: smart-open, bz2file\n",
      "  Running setup.py bdist_wheel for smart-open ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /home/ubuntu/.cache/pip/wheels/36/48/35/97efc2bd1b233627131c9a936c9de23681846db707b907d353\n",
      "  Running setup.py bdist_wheel for bz2file ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /home/ubuntu/.cache/pip/wheels/31/9c/20/996d65ca104cbca940b1b053299b68459391c01c774d073126\n",
      "Successfully built smart-open bz2file\n",
      "Installing collected packages: bz2file, jmespath, botocore, s3transfer, boto3, smart-open, gensim\n",
      "Successfully installed boto3-1.6.6 botocore-1.9.6 bz2file-0.98 gensim-3.4.0 jmespath-0.9.3 s3transfer-0.1.13 smart-open-1.5.6\n"
     ]
    }
   ],
   "source": [
    "#!pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import corpora, models, similarities, matutils\n",
    "tfidf_corpus = matutils.Sparse2Corpus(tfidf_vecs.transpose())\n",
    "\n",
    "id2word = corpora.Dictionary.from_corpus(tfidf_corpus, \n",
    "                                         id2word=id2word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pickle the corpus\n",
    "output = open('tfidf_corpus.pkl', 'wb')\n",
    "pickle.dump(tfidf_corpus, output)\n",
    "\n",
    "output.close()\n",
    "\n",
    "# Pickle the id2word\n",
    "output = open('id2word.pkl', 'wb')\n",
    "pickle.dump(id2word, output)\n",
    "\n",
    "output.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['lsi.pkl']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.externals import joblib\n",
    "\n",
    "lsi = models.LsiModel(tfidf_corpus, id2word=id2word, num_topics=300)\n",
    "\n",
    "joblib.dump(lsi,'lsi.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.externals import joblib\n",
    "import pickle\n",
    "lsi = joblib.load('lsi.pkl')\n",
    "# Load the vectors\n",
    "pkl_file = open('tfidf_corpus.pkl', 'rb')\n",
    "tfidf_corpus= pickle.load(pkl_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "lsi_corpus = lsi[tfidf_corpus]\n",
    "\n",
    "# List of document vectors\n",
    "#doc_vecs = [doc for doc in lsi_corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_vecs[-1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pickle the lsi_corpus\n",
    "output = open('lsi_corpus.pkl', 'wb')\n",
    "pickle.dump(lsi_corpus, output)\n",
    "\n",
    "output.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating similarity\n",
    "\n",
    "I create a similarity matrix so that I get a similarity score for each speech with each other speech. I actually only interested in the mean similarity of each speech with all of the Royal Institution Christmas lectures as a whole.  This mean similarity score for each speech becomes the score for 'scientificness' or 'evidence-basedness'.\n",
    "\n",
    "The similarity with the list of science words is a simplier version of modelling the 'evidence-basedness' in the same way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model\n",
    "lsi = joblib.load('lsi.pkl')\n",
    "\n",
    "# Load the vectors\n",
    "pkl_file = open('lsi_corpus.pkl', 'rb')\n",
    "lsi_corpus= pickle.load(pkl_file)\n",
    "\n",
    "# Load original dataframe if not loaded already\n",
    "pkl_file = open('df_all_science_docs.pkl', 'rb')\n",
    "df_all_science_docs = pickle.load(pkl_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import corpora, models, similarities, matutils\n",
    "index = similarities.MatrixSimilarity(lsi_corpus, \n",
    "                                      num_features=300)\n",
    "\n",
    "sci_docs = df_all_science_docs.index[df_all_science_docs['MP'] == 'Dr Science'].tolist()\n",
    "\n",
    "for doc in sci_docs:\n",
    "    df_all_science_docs['scientificness_{0}'.format(doc)] = index[lsi_corpus[doc]]\n",
    "    \n",
    "scientific_cols = [col for col in df_all_science_docs.columns if 'scientificness' in col]\n",
    "df_all_science_docs['scientificness_avg'] = df_all_science_docs[scientific_cols].mean(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_science_docs['science_words'] = index[lsi_corpus[sci_docs[-1]]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I calculate the number of days since the speech as I'm interested in seeing whether speeches have become more evidence-based over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "date_now = pd.to_datetime('2018-03-08')\n",
    "df_all_science_docs['days_ago'] = [(date_now - pd.to_datetime(date)).days for date in df_all_science_docs['date_1']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>days_ago</th>\n",
       "      <th>date_1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6315.0</td>\n",
       "      <td>2000-11-22 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6315.0</td>\n",
       "      <td>2000-11-22 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6315.0</td>\n",
       "      <td>2000-11-22 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6315.0</td>\n",
       "      <td>2000-11-22 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6315.0</td>\n",
       "      <td>2000-11-22 00:00:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   days_ago               date_1\n",
       "0    6315.0  2000-11-22 00:00:00\n",
       "1    6315.0  2000-11-22 00:00:00\n",
       "2    6315.0  2000-11-22 00:00:00\n",
       "3    6315.0  2000-11-22 00:00:00\n",
       "4    6315.0  2000-11-22 00:00:00"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_all_science_docs[['days_ago','date_1']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pickle sci_docs\n",
    "output = open('sci_docs.pkl', 'wb')\n",
    "pickle.dump(sci_docs, output)\n",
    "\n",
    "output.close()\n",
    "\n",
    "# Pickle the dataframe\n",
    "df_all_science_docs.to_pickle('df_similarity.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
